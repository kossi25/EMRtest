{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Glue database/table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_DyF = glueContext.create_dynamic_frame.from_catalog(database=\"bbb-glue-crawler-taxi-db\", \\\n",
    "                                                        table_name=\"taxidata_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Do basic feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_DyF.toDF().createOrReplaceTempView(\"taxi\")\n",
    "\n",
    "\n",
    "features_DF = spark.sql(\"SELECT PULocationID, DOLocationID, passenger_count, trip_distance, RatecodeID, \\\n",
    "                    total_amount, payment_type, trip_type, fare_amount, \\\n",
    "                    ROUND(CAST(tip_amount/fare_amount AS DOUBLE), 4) as tip_percent, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'yyyy') AS INT) as pickup_year, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'MM') AS INT) as pickup_month,\\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'dd') AS INT) as pickup_day, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'hh') AS INT) as pickup_hour, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'mm') AS INT) as pickup_minute, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'yyyy') AS INT) as dropoff_year, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'MM') AS INT) as dropoff_month,\\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'dd') AS INT) as dropoff_day, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'hh') AS INT) as dropoff_hour, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'mm') AS INT) as dropoff_minute, \\\n",
    "                    ROUND(CAST((unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa') - unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'))/360 AS DOUBLE), 4) as tripdurr, tip_amount \\\n",
    "                    FROM taxi WHERE fare_amount > 2.50\").na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_DF.createOrReplaceTempView(\"taxi\")\n",
    "features_DF = spark.sql(\"SELECT *, \\\n",
    "    ROUND(CAST(trip_distance/tripdurr AS DOUBLE), 4) as avg_speed \\\n",
    "    FROM taxi WHERE pickup_month in (1, 2, 3) AND pickup_year=2017 AND tip_percent<1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def check_airport_id(id):\n",
    "     return int((id == 1) | (id == 2))\n",
    "    \n",
    "check_airport_id_udf = udf(check_airport_id, IntegerType())\n",
    "features_DF = features_DF.withColumn(\"is_airport\", check_airport_id_udf(features_DF['RateCodeID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_DF.createOrReplaceTempView(\"taxi\")\n",
    "features_DF = spark.sql(\"SELECT PULocationID, DOLocationID, passenger_count, trip_distance, RatecodeID, total_amount, payment_type, trip_type, fare_amount, tip_percent, pickup_year, pickup_month, pickup_day, pickup_hour, pickup_minute, dropoff_year, dropoff_month, dropoff_day, dropoff_hour, dropoff_minute, tripdurr, avg_speed, is_airport, tip_amount FROM taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_DF.createOrReplaceTempView(\"taxi\")\n",
    "train_DF = spark.sql(\"SELECT * FROM taxi WHERE pickup_month in (1) and pickup_year=2017\")\n",
    "train_DF = train_DF.drop(\"pickup_year\").drop(\"pickup_month\").drop(\"dropoff_year\").drop(\"dropoff_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_DF = spark.sql(\"SELECT * FROM taxi WHERE pickup_month in (2) and pickup_year=2017\")\n",
    "validation_DF = validation_DF.drop(\"pickup_year\").drop(\"pickup_month\").drop(\"dropoff_year\").drop(\"dropoff_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF = spark.sql(\"SELECT * FROM taxi WHERE pickup_month in (3) and pickup_year=2017\")\n",
    "test_DF = test_DF.drop(\"pickup_year\").drop(\"pickup_month\").drop(\"dropoff_year\").drop(\"dropoff_month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'aws-emr-resources-507786327009-us-east-1'\n",
    "bucket_prefix = 'taxidata_v{}'.format(time.strftime(\"%Y%m%d%H%M%S\", time.gmtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DF.repartition(1).write.csv('s3://{}/{}/train/train.csv'.format(bucket, bucket_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_DF.repartition(1).write.csv('s3://{}/{}/validation/validation.csv'.format(bucket, bucket_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF.repartition(1).write.csv('s3://{}/{}/test/test.csv'.format(bucket, bucket_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Update the bucket prefix in Dynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = boto3.resource('dynamodb', region_name='us-east-1').Table('taxi_training_data_location')\n",
    "#response = table.get_item(Key={'bucketid': 'validation'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esponse=table.update_item(\n",
    "    Key={'bucketid': 'validation'},\n",
    "    UpdateExpression=\"SET prefix= :var1\",\n",
    "    ExpressionAttributeValues={\n",
    "            ':var1': bucket_prefix\n",
    "            },\n",
    "    ReturnValues=\"UPDATED_NEW\"\n",
    "\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
