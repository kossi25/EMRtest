{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Copy the **Credentials / CLI Snippets**.\n",
    "\n",
    "It would look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export AWS_DEFAULT_REGION=us-east-1\n",
    "export AWS_ACCESS_KEY_ID=ASIAT27S2FsT6RHKVT7D\n",
    "export AWS_SECRET_ACCESS_KEY=Fbsb6282OBkMSA/iwpIwvs+crbZ+E2DZOAajmIkk\n",
    "export AWS_SESSION_TOKEN=FwoGZXIvYXdzEIr//////////wEaDEm145FRWPTP33mTeiKuAc2EnC123DtsExywKl7Vixzwg0hqDHgihlFvpgWUjCGhDVIM+PGSp8HwZ4utkcqV0x0asVs/CFdRqQh5OmYPW+uUBr+7DKcByH9ivJ+ze2lLkqkvYBvP8mHeF2ny6fJFufZmKCQy6k4D/jr9tRurufF989XupYWaPBNZ7gTdi7cQOzeW9tquLP+bhUT8hjR11nA+G7VwNTZEY15+WHLaeBMEf73cVORP3sn33FY0yyjptOzzBTIt8xzZKY+fSDlQuQ90GnsnU8NFHTF27t7rAc7fYid4bf2zy++5bUIA3VB/+P8a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Start the cluster with m3.xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Inside the Notebook open a Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws s3 ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl https://aws-emr-resources-25032020-us-east-1.s3.amazonaws.com/taxidata.csv | aws s3 cp - s3://<bucket-name>/taxidata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws s3 ls s3://<bucket-name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The spark code inside the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "from itertools import islice\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col, isnan, when, trim\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**s3distcp**\n",
    "\n",
    "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi = spark.read.format(\"s3selectCSV\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").\\\n",
    "load(\"s3://<bucket_name>/taxidata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(green_taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((green_taxi.count(), len(green_taxi.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do pre-processing in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi.createOrReplaceTempView(\"taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "sqlDF = spark.sql(\"SELECT PULocationID, DOLocationID, passenger_count, trip_distance, RatecodeID, \\\n",
    "                    total_amount, payment_type, trip_type, tip_amount, fare_amount, \\\n",
    "                    ROUND(CAST(tip_amount/fare_amount AS DOUBLE), 4) as tip_percent, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'yyyy') AS INT) as pickup_year, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'MM') AS INT) as pickup_month,\\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'dd') AS INT) as pickup_day, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'hh') AS INT) as pickup_hour, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'mm') AS INT) as pickup_minute, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'yyyy') AS INT) as dropoff_year, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'MM') AS INT) as dropoff_month,\\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'dd') AS INT) as dropoff_day, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'hh') AS INT) as dropoff_hour, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'mm') AS INT) as dropoff_minute, \\\n",
    "                    ROUND(CAST((unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa') - unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'))/360 AS DOUBLE), 4) as tripdurr \\\n",
    "                    FROM taxi WHERE fare_amount > 2.50\")\n",
    "\n",
    "sqlDF.createOrReplaceTempView(\"taxi\")\n",
    "\n",
    "print(f'Total time: {(time.time() - start_time):.3f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = sqlDF.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF.createOrReplaceTempView(\"taxi\")\n",
    "sqlDF = spark.sql(\"SELECT *, \\\n",
    "    ROUND(CAST(trip_distance/tripdurr AS DOUBLE), 4) as avg_speed \\\n",
    "    FROM taxi WHERE pickup_month in (1, 2) AND pickup_year=2017 AND tip_percent<1\").na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SparkML data (feature) transformers\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use UDF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_airport_id(id):\n",
    "     return int((id == 1) | (id == 2))\n",
    "    \n",
    "check_airport_id_udf = udf(check_airport_id, IntegerType())\n",
    "sqlDF = sqlDF.withColumn(\"is_airport\", check_airport_id_udf(sqlDF['RateCodeID']))\n",
    "\n",
    "sqlDF.createOrReplaceTempView(\"taxi\")\n",
    "\n",
    "train = spark.sql(\"SELECT * FROM taxi WHERE pickup_month in (1) and pickup_year=2017\")\n",
    "train = train.drop(\"pickup_year\").drop(\"pickup_month\").drop(\"dropoff_year\").drop(\"dropoff_month\")\n",
    "\n",
    "test = spark.sql(\"SELECT * FROM taxi WHERE pickup_month in (2) and pickup_year=2017\")\n",
    "test = test.drop(\"pickup_year\").drop(\"pickup_month\").drop(\"dropoff_year\").drop(\"dropoff_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.write.csv('nyc_taxi_train.csv')\n",
    "#test.write.csv('nyc_taxi_test.csv')\n",
    "\n",
    "# Save in Parquet files\n",
    "\n",
    "train.write.parquet('nyc_taxi_train.parquet')\n",
    "test.write.parquet('nyc_taxi_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCols = train.columns\n",
    "featuresCols.remove('tip_amount')\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)\n",
    "\n",
    "gbt = GBTRegressor(labelCol=\"tip_amount\")\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxDepth, [2]).addGrid(gbt.maxIter, [2]).build()\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), \\\n",
    "                                predictionCol=gbt.getPredictionCol())\n",
    "\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "pipelineModel = pipeline.fit(train)\n",
    "print(f'Total time: {(time.time() - start_time):.3f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipelineModel.transform(test)\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE on our test set: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark tuning\n",
    "https://spark.apache.org/docs/latest/tuning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
