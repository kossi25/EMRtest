{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "from itertools import islice\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col, isnan, when, trim\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"s3://aws-emr-resources-507786327009-us-east-1/taxidata.csv\"\n",
    "\n",
    "green_taxi = spark.read.format(\"s3selectCSV\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").\\\n",
    "load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(green_taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((green_taxi.count(), len(green_taxi.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do pre-processing in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi.createOrReplaceTempView(\"taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "sqlDF = spark.sql(\"SELECT PULocationID, DOLocationID, passenger_count, trip_distance, \\\n",
    "                    total_amount, payment_type, trip_type, tip_amount, fare_amount, \\\n",
    "                    ROUND(CAST(tip_amount/fare_amount AS DOUBLE), 4) as tip_percent, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'yyyy') AS INT) as pickup_year, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'MM') AS INT) as pickup_month,\\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'dd') AS INT) as pickup_day, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'hh') AS INT) as pickup_hour, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'mm') AS INT) as pickup_minute, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'yyyy') AS INT) as dropoff_year, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'MM') AS INT) as dropoff_month,\\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'dd') AS INT) as dropoff_day, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'hh') AS INT) as dropoff_hour, \\\n",
    "                    CAST(from_unixtime(unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa'), 'mm') AS INT) as dropoff_minute, \\\n",
    "                    ROUND(CAST((unix_timestamp(lpep_dropoff_datetime, 'MM/dd/yyyy hh:mm:ss aa') - unix_timestamp(lpep_pickup_datetime, 'MM/dd/yyyy hh:mm:ss aa'))/360 AS DOUBLE), 4) as tripdurr \\\n",
    "                    FROM taxi WHERE fare_amount > 2.50\")\n",
    "\n",
    "sqlDF.createOrReplaceTempView(\"taxi\")\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f'Total time: {(time.time() - start_time):.3f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_null(c):\n",
    "    return when(~(col(c).isNull() | isnan(col(c)) | (trim(col(c)) == \"\")), col(c))\n",
    "\n",
    "sqlDF = sqlDF.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF.createOrReplaceTempView(\"taxi\")\n",
    "sqlDF = spark.sql(\"SELECT *, \\\n",
    "    ROUND(CAST(trip_distance/tripdurr AS DOUBLE), 4) as avg_speed \\\n",
    "    FROM taxi WHERE pickup_month in (1, 2) AND pickup_year=2017 AND tip_percent<1\").na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"sqlDF = sqlDF.join(zones, sqlDF.PULocationID == zones.LocationID, how='left')\n",
    "sqlDF = sqlDF.drop(\"Zone\").drop(\"service_zone\").drop('LocationID')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Spark ML transformers\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexerBorough = StringIndexer(inputCol=\"Borough\", outputCol=\"PU_boroughIndex\")\n",
    "sqlDF = indexerBorough.fit(sqlDF).transform(sqlDF)\n",
    "sqlDF = sqlDF.drop(\"Borough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = sqlDF.withColumn(\"PU_boroughIndex\", col(\"PU_boroughIndex\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_airport_id(id):\n",
    "     return int((id == 1) | (id == 2))\n",
    "    \n",
    "#check_airport_id_udf = udf(check_airport_id, IntegerType())\n",
    "#sqlDF = sqlDF.withColumn(\"is_airport\", check_airport_id_udf(sqlDF['RateCodeID']))\n",
    "\n",
    "sqlDF.createOrReplaceTempView(\"taxi\")\n",
    "\n",
    "train = spark.sql(\"SELECT * FROM taxi WHERE pickup_month in (1) and pickup_year=2017\")\n",
    "train = train.drop(\"pickup_year\").drop(\"pickup_month\").drop(\"dropoff_year\").drop(\"dropoff_month\")\n",
    "\n",
    "test = spark.sql(\"SELECT * FROM taxi WHERE pickup_month in (2) and pickup_year=2017\")\n",
    "test = test.drop(\"pickup_year\").drop(\"pickup_month\").drop(\"dropoff_year\").drop(\"dropoff_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.write.csv('nyc_taxi_train.csv')\n",
    "test.write.csv('nyc_taxi_test.csv')\n",
    "\n",
    "# Save in Parquet files\n",
    "\n",
    "train.write.parquet('nyc_taxi_train.parquet')\n",
    "test.write.parquet('nyc_taxi_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCols = train.columns\n",
    "featuresCols.remove('tip_amount')\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)\n",
    "\n",
    "gbt = GBTRegressor(labelCol=\"tip_amount\")\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxDepth, [2]).addGrid(gbt.maxIter, [2]).build()\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), \\\n",
    "                                predictionCol=gbt.getPredictionCol())\n",
    "\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipelineModel.transform(test)\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE on our test set: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.install_pypi_package(\"pandas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
